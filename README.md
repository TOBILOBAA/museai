MuseAI â€” Multimodal Museum Companion

A vision-to-voice AI guide that sees, listens, reasons, and speaks.

MuseAI is a fully multimodal museum companion built with:
âœ”ï¸ Gemini Vision â†’ artifact recognition
âœ”ï¸ FAISS RAG â†’ artifact knowledge retrieval
âœ”ï¸ Gemini Flash (LLM) â†’ multilingual reasoning
âœ”ï¸ Google Speech-to-Text â†’ speech recognition
âœ”ï¸ ElevenLabs â†’ multilingual speech generation
âœ”ï¸ Streamlit â†’ interactive camera + audio UI

The system uses a camera snapshot to identify an artifact, listens to your spoken question, retrieves factual context from a vector database, reasons with an LLM, and speaks the answer back in English, French, or Hebrew.

â¸»

ğŸš€ Features

ğŸ” 1. Vision Recognition (Gemini Vision)
	â€¢	Users capture an artifact using the camera.
	â€¢	Model compares image with your artifact dataset (artifacts.csv).
	â€¢	Returns artifact ID, title, confidence, and reasoning.

ğŸ“š 2. RAG (FAISS Vectorstore)
	â€¢	Builds a text-embedding index of artifact metadata using text-embedding-004.
	â€¢	Used for:
	â€¢	Query-based retrieval
	â€¢	Artifact ID â†’ Context retrieval

ğŸ§  3. Multilingual LLM Reasoning
	â€¢	Reasoning powered by Gemini Flash.
	â€¢	Answers in the userâ€™s preferred language.
	â€¢	Safely blends RAG context + LLM knowledge.

ğŸ¤ 4. Speech-to-Text (Google STT)
	â€¢	Supports English, French, Hebrew automatically.
	â€¢	Detects the spoken language and updates UI language dynamically.

ğŸ”Š 5. Text-to-Speech (ElevenLabs v3)
	â€¢	One universal multilingual voice.
	â€¢	Natural responses in EN/FR/HE.

ğŸ’» 6. Streamlit Frontend
	â€¢	Dark, elegant UI.
	â€¢	Camera input, microphone input, real-time chat bubbles, and audio playback.

â¸»


â–¶ï¸ Running the Full App

1. Install dependencies

pip install -r requirements.txt

2. Create your .env

GCP_PROJECT_ID=your_project
GCP_LOCATION=us-central1
GCP_SERVICE_ACCOUNT_JSON="your JSON here"
ELEVENLABS_API_KEY=your_key
VOICE_ID_MULTI=your_voice_id

3. Build the vectorstore (one-time)

python app/rag.py

This creates:
	â€¢	artifacts_index.faiss
	â€¢	artifacts_metadata.parquet

4. Run Streamlit

streamlit run app/streamlit_app.py


â¸»

ğŸ’¡ Development Workflow

MuseAI was built in a modular task-based flow:
	1.	Vision â†’ Identify artifact
	2.	RAG â†’ Build knowledge base
	3.	Voice STT â†’ Understand speech
	4.	Reasoning â†’ Combine all signals
	5.	TTS â†’ Speak response
	6.	UI â†’ Bind everything together
	7.	E2E Tests â†’ Validate functionality on local CLI
	8.	Deployment â†’ Streamlit Cloud + environment fixes

This ensures every component works independently before integration.

â¸»

ğŸ§© Why This Architecture?

We follow a â€œsee â†’ understand â†’ reason â†’ speakâ€ flow:
	â€¢	ğŸ“· Vision first: The app must know what object the user is standing in front of.
	â€¢	ğŸ§  RAG second: Retrieve factual metadata about that object.
	â€¢	ğŸ¤ Voice next: Interpret the userâ€™s spoken question.
	â€¢	ğŸ“ Reasoning: Combine artifact + question into answer.
	â€¢	ğŸ”Š TTS output: Deliver answer as natural audio.

Each module is independently testable and maintainable.

â¸»

ğŸ‘¥ For Developers

Add new artifacts

Edit data/artifacts.csv, then rebuild FAISS index:

python app/rag.py

Swap languages

Update LANGUAGE_CODE_MAP in voice.py, and TTS still works automatically.

Swap LLM model

Change LLM_MODEL_NAME in reasoning.py.

